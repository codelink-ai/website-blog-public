
  
  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  
  
  

  [{"categories":["python"],"contents":"Today we share with you 3 relatively cold knowledge\n The first one: the magic dictionary key  some_dict = {} some_dict[5.5] = \u0026#34;Ruby\u0026#34; some_dict[5.0] = \u0026#34;JavaScript\u0026#34; some_dict[5] = \u0026#34;Python\u0026#34; Output:\n\u0026gt;\u0026gt;\u0026gt; some_dict[5.5] \u0026#34;Ruby\u0026#34; \u0026gt;\u0026gt;\u0026gt; some_dict[5.0] \u0026#34;Python\u0026#34; \u0026gt;\u0026gt;\u0026gt; some_dict[5] \u0026#34;Python\u0026#34; \u0026ldquo;Python\u0026rdquo; eliminates the existence of \u0026ldquo;JavaScript\u0026rdquo;?\nðŸ’¡ Description:\n  The Python dictionary determines whether two keys are identical by checking for key equality and comparing hash values.\n  Immutable objects with the same value always have the same hash value in Python.\n  Note: Objects with different values may also have the same hash (hash collision).\n\u0026gt;\u0026gt; 5 == 5.0 True \u0026gt;\u0026gt;\u0026gt; hash(5) == hash(5.0) True When executing the statement some_dict[5] = \u0026quot;Python\u0026quot;, the existing value \u0026ldquo;JavaScript\u0026rdquo; is overwritten by \u0026ldquo;Python\u0026rdquo; because Python recognizes 5 and 5.0 as the same key of some_dict.\n Second: return in exception handling  def some_func(): try: return \u0026#39;from_try\u0026#39; finally: return \u0026#39;from_finally\u0026#39; Output:\n\u0026gt;\u0026gt;\u0026gt; some_func() \u0026#39;from_finally\u0026#39; ðŸ’¡ Description:\n  When return, break or continue is executed in the try of the \u0026ldquo;try\u0026hellip;finally\u0026rdquo; statement, the finally clause is still executed.\n  The return value of the function is determined by the last executed return statement. Since the finally clause will always be executed, the return in the finally clause will always be the last statement executed.\n   Third: Determination of identical objects  class WTF: pass Output:\n\u0026gt;\u0026gt;\u0026gt; WTF() == WTF() # Two different objects should not be equal False \u0026gt;\u0026gt;\u0026gt; WTF() is WTF() # also not the same False \u0026gt;\u0026gt;\u0026gt; hash(WTF()) == hash(WTF()) # The hash values should also be different True \u0026gt;\u0026gt;\u0026gt; id(WTF()) == id(WTF()) True ðŸ’¡ Description:\n  When the id function is called, Python creates an object of class WTF and passes it to the id function. The id function then gets its id value (that is, its memory address), and discards the object. The object is then destroyed.\n  When we do this twice in a row, Python allocates the same memory address to the second object. Because the id function (in CPython) uses the object\u0026rsquo;s memory address as the object\u0026rsquo;s id value, the id values of both objects are the same.\n  In summary, an object\u0026rsquo;s id value is unique only for the life of the object. After the object is destroyed, or before it is created, other objects can have the same id value.\n  So why does the is operation result in False? Let\u0026rsquo;s look at this code.\n  class WTF(object): def __init__(self): print(\u0026#34;I\u0026#34;) def __del__(self): print(\u0026#34;D\u0026#34;) Output:\n\u0026gt;\u0026gt;\u0026gt; WTF() is WTF() I I D D False \u0026gt;\u0026gt;\u0026gt; id(WTF()) == id(WTF()) I D I D True As you can see, the order of object destruction is the reason for all the differences.\nOriginal link: https://github.com/leisurelicht/wtfpython-cn\n","date":"December 19, 2021","image":"/images/post/Python_logo_hu61b1f0e14e530d3a37400d5ba2f45654_77504_460x200_fill_q75_h2_box_smart1_3.webp","permalink":"/post/python/there-are-3-incredible-return-functions-in-python/","tags":["python","learning"],"title":"There are 3 incredible return functions in Python"},{"categories":["data science"],"contents":"Have you ever thought of animating a child\u0026rsquo;s drawing? As shown below, children can draw unique and creative characters and animals: a star with two legs, a bird with super long legs \u0026hellip;\u0026hellip;\nImage\nParents and teachers can easily understand what a child\u0026rsquo;s drawing is trying to convey, but AI has a hard time with this task because children\u0026rsquo;s drawings are often constructed in abstract and peculiar ways. In the case of a child\u0026rsquo;s drawing of a \u0026ldquo;person,\u0026rdquo; the drawing has many different forms, colors, sizes, and proportions, with few similarities in body symmetry, form, and perspective. It is still difficult for AI to recognize children\u0026rsquo;s drawings.\nMany AI tools and techniques have emerged to handle realistic drawings, but children\u0026rsquo;s drawings add a degree of diversity and unpredictability that complicates the recognition of what is depicted.\nMany AI researchers are trying to overcome this challenge so that AI systems can better recognize the variety of character drawings created by children.\nRecently, Meta announced the first AI system that can automatically animate children\u0026rsquo;s hand-drawn characters and humanoid characters (i.e., characters with two arms, two legs, a head, etc.) with a high success rate without any human guidance, going from a static drawing to an animation in a matter of minutes.\nFor example, if a child draws a kitty and a bee and uploads it to Meta AI, you will see the drawing become a dancing character with very realistic movements.\nImage\nTry it out at https://sketch.metademolab.com/\nBy uploading drawings to the Meta Prototype System, users can experience the transformation of drawings into jumping characters. In addition, users can download the animation and share it with friends and family. If users want, they can also submit these drawings to help improve the AI model.\nMeta does the transition from drawing to animation in four steps: target detection to identify human figures; using character masks to lift human figures from the scene; \u0026ldquo;rigging\u0026rdquo; to prepare for animation; and using 3D motion capture to animate 3D human figures.\nImage\nTarget detection to identify human figure\nThe first step is to distinguish the figure in the drawing from the background and other types of characters in the drawing. The existing target detection method works well for children\u0026rsquo;s drawings, but the segmentation mask is not accurate enough to be used for animation. To solve this problem, Meta instead uses bounding boxes obtained from the target detector and applies a series of morphological operations and image processing steps to obtain the masks.\nMeta AI uses Mask R-CNN, a convolutional neural network-based target detection model, to extract characters from children\u0026rsquo;s drawings. Although Mask R-CNN is pre-trained on the largest segmented dataset, the dataset is composed of photos of real-world objects, not paintings. To enable the model to handle drawings, the model needed to be fine-tuned, and Meta AI was fine-tuned using ResNet-50+FPN to predict a single category of \u0026lsquo;human drawings\u0026rsquo;. Meta AI fine-tuned the model on about 1,000 paintings.\nAfter fine-tuning, the model detected the human figure in the test dataset very well. However, there were also failures, which can be divided into four categories as follows: detecting human figures that do not contain the whole image (e.g., the tail is not included in the figure); not separating human figures from the background; not separating several human figures that are grouped together; and incorrectly identifying non-human figures (e.g., trees).\nImage\nUse character mask to lift human figures from the scene\nAfter identifying and extracting the figure from the drawing, the next step in generating the animation is to separate it from the rest of the scene and the background, a process called masking. mask must accurately map the figure\u0026rsquo;s outline, as it will be used to create the mesh, and then morph it to generate the animation. Once everything is in place, the mask will contain all the components of the character and eliminate any background content.\nAlthough Mask R-CNNs can output masks, Meta AI found that they are not suitable for animation. When the appearance of body parts varies a lot, the predicted mask usually does not capture the whole character. As shown in the bottom row of the example below, a large yellow triangle represents the body and a pencil stroke represents the arm. When using Mask R-CNN to predict the mask, the pencil stroke part connecting the hands is usually missed.\nBased on this, Meta AI developed a method based on classical image processing that is more robust to character changes. Based on this approach, Meta AI crops the image using the predicted bounding box of the human figure. Then, using adaptive thresholding and morphological closing/dialating operations, the edges of the bounding box are filled from, and the mask is assumed to be the largest polygon that is not filled.\nImage\nMask R-CNN compares the results with those based on classical image processing methods.\nHowever, while this method is simple and effective for extracting the exact mask suitable for the animation, it may also fail when the background is cluttered, when the characters are too close together, or when there are creases and tears or shadows on the paper page.\nPreparing for animation by \u0026ldquo;rigging\nChildren can draw a wide variety of body shapes that go far beyond the traditional concept of a human figure with a full head, arms, legs and torso. Some children draw Tinder figures without a torso, with only arms and legs directly connected to the head. Other children drew even weirder human figures, with legs extending from the head and arms from the eyes of the thighs.\nTherefore, Meta AI needed to find a rigging method that would allow for the change of body shape.\nImage\nThey chose to use AlphaPose, a human pose detection model, to identify key points in human drawings as hips, shoulders, elbows, knees, wrists, and ankles. The model was trained on live images, so Meta AI had to be retrained to handle the types of variation present in the children\u0026rsquo;s drawings before it could be adapted to detect the human pose in the children\u0026rsquo;s drawings.\nSpecifically, Meta AI achieves this goal by internally collecting and annotating small datasets of children\u0026rsquo;s humanoid pictures. The pose detectors trained on these initial datasets were then used to create an internal tool that allowed parents to upload and animate their children\u0026rsquo;s drawings. As more data is added, Meta AI iteratively retrains the model until a high level of accuracy is achieved.\n3D humanoid animation using 3D motion capture\nWith masking and joint prediction, everything is needed to animate. Meta AI first uses the extracted mask to generate a mesh and texturizes it with the original drawing. Using the predicted joint positions, they create the skeleton for the character. Afterwards, the character is ported into various poses by rotating the bones and deforming the mesh using the new joint positions. By transposing the character into a series of consecutive poses, the animation can then be created.\nIt is common for children to draw body parts from their most recognizable angles, such as the tendency to draw legs and feet from the side and the head and torso from the front. For the lower body and the raw body, they automatically determine whether to recognize the action from the front or the side.\nSpecifically, they map the action to a single 2D plane and use it to drive the character, and validate the results of this action repositioning using a perceptual user study run by Mechanical Turk. The segmentation detection process is shown in the following figure.\nImage\nMeta AI says it is helpful to take distorted views into account because many types of movements do not finish on a single projection plane. For example, when jumping rope, the arms and wrists move primarily in the frontal plane, while bent legs tend to move in the sagittal plane. Therefore, Meta AI does not determine individual motion platforms for motion capture poses, but rather separate projection planes for the upper and lower body.\nMeanwhile, with AR eyes, the story in the painting can come to life in the real world, and the character in the painting can even dance or talk with the child who drew it.\nLink to original article: https://ai.facebook.com/blog/using-ai-to-bring-childrens-drawings-to-life/\n","date":"December 18, 2021","image":"/images/post/Python_logo_hu61b1f0e14e530d3a37400d5ba2f45654_77504_460x200_fill_q75_h2_box_smart1_3.webp","permalink":"/post/data-science/meta-ai-has-created-a-wonderful-world-of-stickman/","tags":["data science","deep learning"],"title":"Meta AI has created a wonderful world of stickman"},{"categories":["photography"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 \rEmphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\n\rLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\n\rParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\n\rOrdered List  List item List item List item List item List item  \rUnordered List  List item List item List item List item List item  \rNotice This is a simple note.\n This is a simple tip.\n This is a simple info.\n \rTab  This is first tab \rthis is second tab \rthis is third tab \r  \rCollapse collapse 1   This is a simple collapse  collapse 2   This is a simple collapse  collapse 3   This is a simple collapse  \rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s \rBlockquote  This is a blockquote example.\n \rInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\n\rDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\r\r\rTables Colons can be used to align columns.\n   Tables Are Cool     col 3 is right-aligned $1600   col 2 is centered $12   zebra stripes are neat $1    There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\n   Markdown Less Pretty     Still renders nicely   1 2 3    \rImage \rYoutube video   ","date":"March 15, 2020","image":"/images/post/post-6_hue652d174a668504e08bd47e88a7e152b_295919_460x200_fill_q75_h2_box_smart1.webp","permalink":"/post/elements/","tags":["photo","image"],"title":"Elements That You Can Use To Create A New Post On This Template."}]